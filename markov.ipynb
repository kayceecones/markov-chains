{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams and Markov Chains for a Girl\n",
    "\n",
    "First, read our input text from a file and clean it into a list of words without special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "def clean(input_text):\n",
    "    result = input_text\n",
    "\n",
    "    special_chars = [\".\", \"\\n\", \";\", \"?\", \"!\", \":\", \",\", \"(\", \")\", \"[\", \"]\", \"\\\"\", \"“\", \"”\", \"*\"]\n",
    "\n",
    "    for char in special_chars:\n",
    "        result = result.replace(char, \" \" if char == \"\\n\" else \"\")\n",
    "\n",
    "    return result.lower()\n",
    "\n",
    "\n",
    "# Clean up the input text\n",
    "def split_and_dropnulls(input_text):\n",
    "    words = input_text.split(\" \")\n",
    "\n",
    "    non_empty_words = [word for word in words if word != '']\n",
    "    return non_empty_words\n",
    "\n",
    "with open('alice.txt', 'r') as alice_file: \n",
    "    alice_text = ' '.join(alice_file.readlines())\n",
    "\n",
    "alice_words = split_and_dropnulls(clean(alice_text))\n",
    "pprint(alice_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's split the text into pairs of two (later, N) words at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_pairs = [(alice_words[i], alice_words[i+1]) for i in range(len(alice_words)-1)]\n",
    "pprint(alice_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can find the frequency of each pair within the set of pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = Counter(alice_pairs)\n",
    "\n",
    "frequencies = pair_counts.most_common(10)\n",
    "pprint(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training! The final model has size: 1310808 bytes\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "\n",
    "def markov_model(sequence: list, n: int = 2):\n",
    "    \"\"\"\n",
    "    Create a Markov model (represented as a dict) from the given input sequence, \n",
    "    using N-grams of size {n}\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    sequence = list(sequence[:]) + [None]\n",
    "    for starting_position in range(len(sequence) - n):\n",
    "        current_ngram = tuple(sequence[starting_position:starting_position + n])\n",
    "        next_item = sequence[starting_position + n]\n",
    "        \n",
    "        if current_ngram not in model: \n",
    "            model[current_ngram] = [next_item]\n",
    "        else:\n",
    "            model[current_ngram].append(next_item)\n",
    "\n",
    "    return model\n",
    "\n",
    "alice_model = markov_model(alice_text, 5)\n",
    "print(f'Finished training! The final model has size: {sys.getsizeof(alice_model)} bytes')\n",
    "# pprint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a Markov model of our text, we can use it to generate more text that \"looks like\" the source material:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pear the flurry tone, and say but it, old fellow!” And oh! sh!” she did then I breath, and walked out, after a fall, at a red-hot possible to hers before she hedgehogs were stood nearly follow, as a large saucer of the love).\n",
      " Oh dear quietly so,” said the Mock Turtles and the temper,” said the Queen’s voice:—\n",
      " \n",
      " “Beautiful Soo—oop of that the table and she such a new idea of having it. She took and was quite crowded round the other!”\n",
      " \n",
      " “That’s the Caterpillar.\n",
      " \n",
      " The Cat; and in a thinking to say “How confusion,” waving way to eat hurried Alice time). “Do bats eat or drinking on both bite. And she same sigh: she think how eagerly, found it please, which was only this, and the Dormouse.\n",
      " \n",
      " The Gryphon went on a tone, and was not,” said Alice thought Alice: “because had plenty of keeping about in a very now—Don’t you, won’t belongs to get away, and so confusion, and went on for sneeze so.” said the other Alice replied very long silent, all difficulty Alice again introduced the e—e—evening down feet, I don’t know?”\n",
      " \n",
      " As she middle of history,”\n",
      " \n",
      " Down the table dog nearly followed it began to changed severely tone.\n",
      " \n",
      " The Caterpillar the glass table for you,” (she was, I shall I know some minute or two she set they cried away!”\n",
      " \n",
      " So Alice ledge of the top of it in an ignorant think?”\n",
      " \n",
      " “Of court was the wouldn’t seem to one such a trumpet, and more, and the made out on a little queer to the question dropping merrily along broken on one replied ever, she’s plenty at altogether.”\n",
      " \n",
      " “Everybody look like,” said Alice and the March.” As she thought Alice, in a hoarsely as she three in some to the too much!”\n",
      " \n",
      " “Oh, don’t explanation.”\n",
      " \n",
      " “Ah! that! No, that are older as he said not make out then for a mouse of the e—e—evening,\n",
      "     Allow a moment Alice said their simple and make the opened the supple\n",
      " By the Queen’s Croquet-ground it out they would bread-and-butter getting on her voice, serious nose, and this, but here isn’t done way offended tone: “don’t know, and be quit"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate(n, model, start=None, max_length=100):\n",
    "    if start is None:\n",
    "        start = random.choice(list(model.keys()))\n",
    "    \n",
    "    output = list(start)\n",
    "\n",
    "    for i in range(max_length):\n",
    "        start = tuple(output[-n:])\n",
    "        next_item = random.choice(model[start])\n",
    "\n",
    "        if next_item is None:\n",
    "            break\n",
    "        else:\n",
    "            output.append(next_item)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "alice_result = generate(5, alice_model, max_length=2000)\n",
    "for char in alice_result:\n",
    "    print(char, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training! The final model has size: 5242968 bytes\n"
     ]
    }
   ],
   "source": [
    "with open('bible.txt', 'r') as bible_file: \n",
    "    bible_text = ' '.join(bible_file.readlines())\n",
    "\n",
    "# Train a model on it \n",
    "bible_model = markov_model(bible_text, n=5)\n",
    "\n",
    "print(f'Finished training! The final model has size: {sys.getsizeof(bible_model)} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rd; and do\n",
      " uncircumcised, and the came them all; for thus I will I rise up Elijah, and\n",
      " took Sosthenes, that following the house of Jedaiah their\n",
      " sea: this pleased me, saith, Rachel and a drink a little bottles all the\n",
      " take down\n",
      " to him, Run. The LORD fountain, Because I had through ye shut upon her; and girt about the came down and be cutteth his name outward when a books of Christ be wrough their shall driven unto him, and return the people, and whom I will generation shall\n",
      " our God for\n",
      " them that thee have slandered him as seek me\n",
      " be remaineth not have been all thine heat off\n",
      " to do great.\n",
      " \n",
      " 7:4 Then have branch thee, O God, without garrison her hand upon the Ethiopians against thine hand of persecuted up\n",
      " and the green to make hasten my souls the times;\n",
      " the mountain of the carry high he door of Judah and\n",
      " Naamath thy sacrifice.\n",
      " \n",
      " 9:22 Whoso to the field, and unto Tahpenest gatherefore him a\n",
      " cloud.\n",
      " \n",
      " 16:34 Ascribed; I would not\n",
      " the sight, that hast loveth great.\n",
      " \n",
      " 2:8 And I an"
     ]
    }
   ],
   "source": [
    "# Generate some more \"bible\" with it\n",
    "bible_result = generate(n=5, model=bible_model, max_length=1000)\n",
    "\n",
    "for char in bible_result:\n",
    "    print(char, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do it to the Grateful Dead. (Do you know the grateful dead?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training! The final model has size: 589920 bytes\n"
     ]
    }
   ],
   "source": [
    "with open('gdead.txt', 'r') as dead_file: \n",
    "    dead_text = ' '.join(dead_file.readlines())\n",
    "\n",
    "# Train a model on it \n",
    "dead_model = markov_model(dead_text, n=5)\n",
    "\n",
    "print(f'Finished training! The final model has size: {sys.getsizeof(dead_model)} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screen, be a judgement I said' \"Please, I am on me.\n",
      " Got some pretty little was clear throw morning for his choice, mate of thing.\n",
      " But it's one thing a mate you, what I've bottle was dusty but throw me in an eagle on ten dollars bail,\n",
      " Mumblin' down,\n",
      " I turning on a life was many angles you pushed around here the rails we're on horseback and the bone,\n",
      " It's the bells on my knees,\n",
      " You three days just there I am on me\".\n",
      " Come heart. You just stumbles as the bastard barely time go?\n",
      " \n",
      " Off to move, rollin' down,\n",
      " And I chundercloud.\n",
      " \n",
      " I live in and the high noon?\n",
      " Like all ugly and rage\n",
      " A lady of nobility, gentility and play and rage\n",
      " A lady of nobility and fill my pillow you with harder to blame.\n",
      " \n",
      " Teeth big split, and warm in you holding on the back...He's gone, nothin' every day.\n",
      " \n",
      " Been all I had a harden wings go wrong, wrong.\n",
      " And sing me a violin an empty, find out with me,\n",
      " Gentle Jack, there quick, yes I comes around of town off the big and now he's gone, he's gone, gone.\n",
      " \n",
      " Nothi"
     ]
    }
   ],
   "source": [
    "# Generate some more \"dead songs\" with it\n",
    "dead_result = generate(n=5, model=dead_model, max_length=1000)\n",
    "\n",
    "for char in dead_result:\n",
    "    print(char, end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
